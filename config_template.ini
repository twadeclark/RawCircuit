[article_selector]
use_info_from_aggregator_instead_of_fetch = true
use_specific_article = false
specific_article_url =
specific_article_title =
specific_article_author =


[model_info]
max_tokens = 1400

# supported: HuggingFaceInterface, LocalOpenAIInterface, TransformersInterface
### note: TransformersInterface downloads full model to local
interface = HuggingFaceInterface

# leave blank for random model from HuggingFace API, except for LocalOpenAIInterface
### note: not used to LocalOpenAIInterface. it just uses whatever model is running in the local server
# looks like this model_name = OpenAssistant/oasst-sft-1-pythia-12b
model_name = 


[general_configurations]
db_type = TinyDB
prompt_tier = creative_style_color
continuity_multiplier = 1.5
number_of_models_to_try_per_aticle = 5
min_comment_temperature = 0.0
max_comment_temperature = 2.0
number_of_comments_between_min_max_temperature = 3
summary_temperature = 1.0
articles_total_qty = 3

[TinyDB]
db_path = .\\db.json

[postgresql]
host=
database=
user=
password=

[NewsAPI]
apiKey =
category = technology
news_search_term = (artifical AND intelligence) OR (machine AND learning) OR ChatGPT OR robot
days_back = 2
sortBy = relevancy

[RSSFeeder]
feed_url =

[HuggingFace]
base_url = https://api-inference.huggingface.co/models/
api_key =

[LocalOpenAIInterface]
base_url =
api_key = unneeded

[TransformersInterface]
base_url = unneeded
api_key = unneeded

[LiteLLMInterface]
base_url = unneeded
api_key = unneeded

[publishing_details]
publish_to_pelican = false
local_content_path  =
local_pelicanconf  =
local_publish_path =

[aws_s3_bucket_details]
publish_to_s3 = false
bucket_name =
s3_directory =
aws_access_key_id =
aws_secret_access_key =
region_name =
